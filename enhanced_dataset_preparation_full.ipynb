{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8651400c",
   "metadata": {},
   "source": [
    "# Enhanced Dataset Preparation for Study Notes Generator\n",
    "\n",
    "We build **evaluation datasets** for summarization and Q&A, focusing on AI/ML/scientific domain.\n",
    "We retrieve Papers, Q&A, Definitions from reputable sources (6 different dataset types) like \n",
    "\n",
    "- ML-ArXiv (300)\n",
    "- real StackOverflow questions with accepted answers filtered to AI/ML/Data Science (250)\n",
    "- PubMedQA (200) \n",
    "- StackOverflow ML Interview Q&A Library(400)\n",
    "- We made custom glossary with 115 items\n",
    "\n",
    "We are covering:\n",
    "- Research-level summaries (ML papers)\n",
    "- Applied QA in ML engineering + data science\n",
    "- Scientific/clinical QA for “AI in healthcare” \n",
    "\n",
    "**Given all the constraints and utilizing a non-script HF datasets to avoid issues, we have a decently extensive and reliable database with 1265 items totally.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0129c",
   "metadata": {},
   "source": [
    "## Step 0 – Imports & Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amulyajayanti/Desktop/NLP_final/nlpfinal./lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED DATASET COLLECTION - AI/ML/DATA SCIENCE FOCUS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"datasets/raw\", exist_ok=True)\n",
    "os.makedirs(\"datasets/cleaned\", exist_ok=True)\n",
    "os.makedirs(\"datasets/custom\", exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENHANCED DATASET COLLECTION - AI/ML/DATA SCIENCE FOCUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cleaned_ml_arxiv = []\n",
    "cleaned_qasper = []\n",
    "cleaned_pubmed = []\n",
    "cleaned_stack = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456a07",
   "metadata": {},
   "source": [
    "## Step 1 – ML-ArXiv Summarization Dataset (`ccdv/arxiv-summarization`)\n",
    "\n",
    "This dataset provides Machine Learning / CS research papers with their **abstracts** as reference\n",
    "summaries. We will:\n",
    "\n",
    "1. Load a small slice of the `test` split.\n",
    "2. Filter out short documents/abstracts.\n",
    "3. Truncate long documents.\n",
    "4. Save as `datasets/cleaned/summarization_ml_arxiv.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dedd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1. Loading ML-ArXiv Summarization Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 203037/203037 [00:24<00:00, 8343.05 examples/s] \n",
      "Generating validation split: 100%|██████████| 6436/6436 [00:02<00:00, 2713.06 examples/s]\n",
      "Generating test split: 100%|██████████| 6440/6440 [00:02<00:00, 2906.40 examples/s]\n",
      "Processing ML-ArXiv: 100%|██████████| 300/300 [00:00<00:00, 11659.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 300 ML/CS-specific papers\n",
      "   File: datasets/cleaned/summarization_ml_arxiv.json\n",
      "   Domain: 100% Machine Learning & Computer Science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n 1. Loading ML-ArXiv Summarization Dataset...\")\n",
    "\n",
    "try:\n",
    "    ml_arxiv = load_dataset(\"ccdv/arxiv-summarization\", split=\"test[:300]\")\n",
    "\n",
    "    cleaned_ml_arxiv = []\n",
    "    for i, item in enumerate(tqdm(ml_arxiv, desc=\"Processing ML-ArXiv\")):\n",
    "        article = item.get(\"article\", \"\")\n",
    "        abstract = item.get(\"abstract\", \"\")\n",
    "\n",
    "        if len(article) > 500 and len(abstract) > 50:\n",
    "            cleaned_ml_arxiv.append({\n",
    "                \"id\": f\"ml_arxiv_{i}\",\n",
    "                \"document\": article[:15000],\n",
    "                \"reference_summary\": abstract,\n",
    "                \"document_type\": \"ml_cs_paper\",\n",
    "                \"length\": len(article),\n",
    "                \"summary_length\": len(abstract)\n",
    "            })\n",
    "\n",
    "    out_path = \"datasets/cleaned/summarization_ml_arxiv.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(cleaned_ml_arxiv[:300], f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(cleaned_ml_arxiv[:300])} ML/CS-specific papers\")\n",
    "    print(f\"   File: {out_path}\")\n",
    "    print(\"   Domain: 100% Machine Learning & Computer Science\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ML-ArXiv: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd91f70",
   "metadata": {},
   "source": [
    "## Step 2 – Real StackOverflow questions with accepted answers filtered to AI/ML/Data Science topics by keyword\n",
    "\n",
    "`stackoverflow` contains questions and answers grounded in AI/ML/Data science discussions. We attempt to:\n",
    "\n",
    "1. Load a slice of the `validation` split.\n",
    "2. Extract a short **context** from the text.\n",
    "3. Extract up to 2 question–answer pairs per discussion.\n",
    "4. Save them as `datasets/cleaned/qa_stackoverflow_ml_250.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c1d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2. Loading StackOverflow AI/ML/Data Science Q&A (250 examples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 15451/15451 [00:00<00:00, 283702.63 examples/s]\n",
      "Filtering ML/DS questions:   3%|▎         | 498/15451 [00:00<00:01, 13214.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 250 AI/ML/DS StackOverflow Q&A examples\n",
      "   File: datasets/cleaned/qa_stackoverflow_ml_250.json\n",
      "   Domain: Practical AI/ML/Data Science programming Q&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"datasets/cleaned\", exist_ok=True)\n",
    "\n",
    "print(\"\\n 2. Loading StackOverflow AI/ML/Data Science Q&A (250 examples)...\")\n",
    "\n",
    "try:\n",
    "    # Full sample from StackOverflow: Q + accepted answer\n",
    "    so_qa = load_dataset(\"eshangj/stackoverflow_q_and_a_sample\", split=\"train\")\n",
    "\n",
    "    # Heuristic keywords for AI/ML/Data Science / Python-for-DS\n",
    "    ml_keywords = [\n",
    "        \"machine learning\", \"deep learning\", \"neural network\",\n",
    "        \"pytorch\", \"tensorflow\", \"keras\", \"scikit-learn\", \"sklearn\",\n",
    "        \"xgboost\", \"lightgbm\", \"catboost\",\n",
    "        \"regression\", \"classification\", \"clustering\",\n",
    "        \"gradient boosting\", \"random forest\",\n",
    "        \"data science\", \"data scientist\",\n",
    "        \"pandas\", \"dataframe\", \"numpy\",\n",
    "        \"matplotlib\", \"seaborn\", \"plotly\",\n",
    "        \"nlp\", \"natural language processing\",\n",
    "        \"transformer\", \"bert\", \"gpt\",\n",
    "        \"time series\", \"forecasting\",\n",
    "        \"cross-validation\", \"train_test_split\",\n",
    "    ]\n",
    "\n",
    "    def is_ml_related(text: str) -> bool:\n",
    "        text = (text or \"\").lower()\n",
    "        return any(kw in text for kw in ml_keywords)\n",
    "\n",
    "    cleaned_so_ml = []\n",
    "    for i, item in enumerate(tqdm(so_qa, desc=\"Filtering ML/DS questions\")):\n",
    "        q = item.get(\"question\", \"\") or \"\"\n",
    "        a = item.get(\"accepted_answer\", \"\") or \"\"\n",
    "\n",
    "        # Need both Q and accepted answer\n",
    "        if not q.strip() or not a.strip():\n",
    "            continue\n",
    "\n",
    "        # Filter to ML/DS-ish content\n",
    "        if not (is_ml_related(q) or is_ml_related(a)):\n",
    "            continue\n",
    "\n",
    "        cleaned_so_ml.append({\n",
    "            \"id\": f\"so_ml_{item.get('question_id', i)}\",\n",
    "            \"question\": q.strip(),\n",
    "            \"answer\": a.strip(),\n",
    "            \"link\": item.get(\"link\", \"\"),\n",
    "            \"question_vote\": int(item.get(\"question_vote\", 0)),\n",
    "            \"answer_vote\": int(item.get(\"answer_vote\", 0)),\n",
    "            \"document_type\": \"stackoverflow_ml_qa\"\n",
    "        })\n",
    "\n",
    "        # Stop once we have 250\n",
    "        if len(cleaned_so_ml) >= 250:\n",
    "            break\n",
    "\n",
    "    out_path = \"datasets/cleaned/qa_stackoverflow_ml_250.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(cleaned_so_ml, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(cleaned_so_ml)} AI/ML/DS StackOverflow Q&A examples\")\n",
    "    print(f\"   File: {out_path}\")\n",
    "    print(\"   Domain: Practical AI/ML/Data Science programming Q&A\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading StackOverflow ML Q&A: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4eeeb",
   "metadata": {},
   "source": [
    "## Step 3 – PubMedQA (Medical/Scientific Q&A)\n",
    "\n",
    "We load `pubmed_qa` with the `pqa_labeled` config and construct a cleaned scientific Q&A dataset:\n",
    "\n",
    "- Question: research question.\n",
    "- Context: small snippet from the `contexts` field.\n",
    "- Answer: `long_answer` (explanatory answer).\n",
    "\n",
    "Saved as `datasets/cleaned/qa_pubmed.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd48517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3. Loading PubMedQA (Medical/Scientific Q&A)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 150016.24 examples/s]\n",
      "Processing PubMedQA: 100%|██████████| 400/400 [00:00<00:00, 17996.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 medical/scientific Q&A pairs\n",
      "   File: datasets/cleaned/qa_pubmed.json\n",
      "   Domain: Medical research (scientific reasoning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n 3. Loading PubMedQA (Medical/Scientific Q&A)...\")\n",
    "\n",
    "try:\n",
    "    pubmed_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train[:400]\")\n",
    "\n",
    "    cleaned_pubmed = []\n",
    "    for i, item in enumerate(tqdm(pubmed_dataset, desc=\"Processing PubMedQA\")):\n",
    "        context = item.get('context', {})\n",
    "        question = item.get('question', '')\n",
    "        long_answer = item.get('long_answer', '')\n",
    "        final_decision = item.get('final_decision', '')\n",
    "\n",
    "        context_text = \"\"\n",
    "        if isinstance(context, dict):\n",
    "            contexts = context.get('contexts', [])\n",
    "            if contexts:\n",
    "                context_text = \" \".join(contexts[:3])\n",
    "\n",
    "        if question and long_answer and len(long_answer) > 50:\n",
    "            cleaned_pubmed.append({\n",
    "                \"id\": f\"pubmed_{i}\",\n",
    "                \"question\": question,\n",
    "                \"context\": context_text[:1500] if context_text else \"See abstract\",\n",
    "                \"answer\": long_answer,\n",
    "                \"decision\": final_decision,\n",
    "                \"type\": \"explanatory_scientific\",\n",
    "                \"domain\": \"medical_research\"\n",
    "            })\n",
    "\n",
    "    out_path = \"datasets/cleaned/qa_pubmed.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(cleaned_pubmed[:200], f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(cleaned_pubmed[:200])} medical/scientific Q&A pairs\")\n",
    "    print(f\"   File: {out_path}\")\n",
    "    print(\"   Domain: Medical research (scientific reasoning)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PubMedQA: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afcd52",
   "metadata": {},
   "source": [
    "## Step 4 – StackOverflow ML Q&A (Bonus Technical Q&A)\n",
    "\n",
    "We attempt to load the **StackOverflow ML Libraries Q&A** library.\n",
    "\n",
    "1. Load a slice of the `train` split.\n",
    "2. Keep entries with a non-trivial question (`title`) and body (`body`).\n",
    "3. Truncate the body to a manageable length.\n",
    "4. Save as `datasets/cleaned/qa_aiml_concepts_400.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab95b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:62: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/cz/16rxwm6j2255gn5yyhbwzbjw0000gn/T/ipykernel_71885/4133827437.py:62: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"\\ Saved AI/ML conceptual Q&A dataset\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Loading AI/ML/NLP Conceptual Q&A (StackOverflow- ML libraries Q&A)...\n",
      "Loading StackOverflow ML library Q&A (Syed-Hasan-8503/StackOverflow-ML-Libraries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StackOverflow ML-Libraries: 100%|██████████| 400/400 [00:00<00:00, 34369.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ Collected 400 Q&A from StackOverflow ML-Libraries\n",
      "\\ Saved AI/ML conceptual Q&A dataset\n",
      "   File     : datasets/cleaned/qa_aiml_concepts_400.json\n",
      "   Total QA : 400\n",
      "   Source: Syed-Hasan-8503/StackOverflow-ML-Libraries (ML libraries Q&A)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"datasets/cleaned\", exist_ok=True)\n",
    "\n",
    "print(\"\\n4. Loading AI/ML/NLP Conceptual Q&A (StackOverflow- ML libraries Q&A)...\")\n",
    "\n",
    "cleaned_aiml_qa = []\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. StackOverflow ML Libraries (Python/ML/Numpy/Pandas/TensorFlow/PyTorch)\n",
    "#    Dataset: Syed-Hasan-8503/StackOverflow-ML-Libraries\n",
    "#    Fields:\n",
    "#       - title\n",
    "#       - question (full body)\n",
    "#       - answer  (accepted/best answer)\n",
    "#       - tags, score\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    print(\"Loading StackOverflow ML library Q&A (Syed-Hasan-8503/StackOverflow-ML-Libraries)...\")\n",
    "    so_ml = load_dataset(\n",
    "        \"Syed-Hasan-8503/StackOverflow-ML-Libraries\",\n",
    "        split=\"train[:400]\"\n",
    "    )\n",
    "\n",
    "    start_idx = len(cleaned_aiml_qa)\n",
    "    for j, ex in enumerate(tqdm(so_ml, desc=\"Processing StackOverflow ML-Libraries\")):\n",
    "        title = (ex.get(\"title\") or \"\").strip()\n",
    "        question_body = (ex.get(\"question\") or \"\").strip()\n",
    "        answer_body = (ex.get(\"answer\") or \"\").strip()\n",
    "        tags = (ex.get(\"tags\") or \"\").strip()\n",
    "        score = int(ex.get(\"score\") or 0)\n",
    "\n",
    "        if not title or not question_body or not answer_body:\n",
    "            continue\n",
    "\n",
    "        cleaned_aiml_qa.append({\n",
    "            \"id\": f\"stackoverflow_ml_{j}\",\n",
    "            \"source\": \"stackoverflow_ml_libraries\",\n",
    "            \"question\": title,          # short question summary\n",
    "            \"context\": question_body,   # full StackOverflow question text\n",
    "            \"answer\": answer_body,      # accepted/best answer\n",
    "            \"tags\": tags,\n",
    "            \"score\": score,\n",
    "            \"domain\": \"python_ml_libraries\"\n",
    "        })\n",
    "\n",
    "    so_count = len(cleaned_aiml_qa) - start_idx\n",
    "    print(f\"  ➜ Collected {so_count} Q&A from StackOverflow ML-Libraries\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading StackOverflow ML-Libraries: {e}\")\n",
    "\n",
    "out_path = \"datasets/cleaned/qa_aiml_concepts_400.json\"\n",
    "final_subset = cleaned_aiml_qa[:400]\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_subset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved AI/ML conceptual Q&A dataset\")\n",
    "print(f\"   File     : {out_path}\")\n",
    "print(f\"   Total QA : {len(final_subset)}\")\n",
    "print(\"   Source: Syed-Hasan-8503/StackOverflow-ML-Libraries (ML libraries Q&A)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bddc7c2",
   "metadata": {},
   "source": [
    "## Step 5 – Create Dataset Summary JSON\n",
    "\n",
    "We now construct a high-level `datasets/DATASET_SUMMARY.json` that:\n",
    "\n",
    "- Inspects which cleaned dataset files actually exist.\n",
    "- Records their file paths, counts, and intended use.\n",
    "- Reports overall counts for quick reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a2cae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5. Creating Dataset Summary...\n",
      "Included ml_arxiv: 300 items from datasets/cleaned/summarization_ml_arxiv.json\n",
      "Included pubmed_qa: 200 items from datasets/cleaned/qa_pubmed.json\n",
      "Included stackoverflow_ml_250: 250 items from datasets/cleaned/qa_stackoverflow_ml_250.json\n",
      "Included aiml_concepts_400: 400 items from datasets/cleaned/qa_aiml_concepts_400.json\n",
      "\n",
      "======================================================================\n",
      "DATASET SUMMARY CREATED\n",
      "======================================================================\n",
      "Summary file: datasets/DATASET_SUMMARY.json\n",
      "Included datasets:\n",
      " - [summarization] ml_arxiv: 300 items -> datasets/cleaned/summarization_ml_arxiv.json\n",
      " - [qa_generation] pubmed_qa: 200 items -> datasets/cleaned/qa_pubmed.json\n",
      " - [qa_generation] stackoverflow_ml_250: 250 items -> datasets/cleaned/qa_stackoverflow_ml_250.json\n",
      " - [qa_generation] aiml_concepts_400: 400 items -> datasets/cleaned/qa_aiml_concepts_400.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 5. Creating Dataset Summary...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    \"dataset_overview\": {\n",
    "        \"domain_focus\": \"AI, Machine Learning, Data Science, Healthcare\",\n",
    "        \"purpose\": \"Evaluation & benchmarking for Personalized Study Notes Generator\",\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"summarization\": {},   # e.g., ML-ArXiv\n",
    "        \"qa_generation\": {},   # PubMed + AI/ML Q&A\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def add_dataset_if_exists(group, key, path, relevance, use_case, dtype):\n",
    "    \"\"\"\n",
    "    group  : \"summarization\" or \"qa_generation\"\n",
    "    key    : short name like \"ml_arxiv\" or \"pubmed_qa\"\n",
    "    path   : JSON file path\n",
    "    dtype  : type label, e.g., \"ml_cs_papers\" or \"medical_qa\"\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            count = len(data)\n",
    "            summary[\"datasets\"][group][key] = {\n",
    "                \"file\": path,\n",
    "                \"count\": count,\n",
    "                \"relevance\": relevance,\n",
    "                \"use_case\": use_case,\n",
    "                \"type\": dtype,\n",
    "            }\n",
    "            print(f\"Included {key}: {count} items from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {path} for summary: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {key}: file not found at {path}\")\n",
    "\n",
    "\n",
    "# ---------------- Summarization datasets ---------------- #\n",
    "\n",
    "# ML-ArXiv summarization (Step 2)\n",
    "add_dataset_if_exists(\n",
    "    group=\"summarization\",\n",
    "    key=\"ml_arxiv\",\n",
    "    path=\"datasets/cleaned/summarization_ml_arxiv.json\",\n",
    "    relevance=\"High (ML/CS research domain)\",\n",
    "    use_case=\"Summarization of ML/CS research articles (article → abstract)\",\n",
    "    dtype=\"ml_cs_papers\",\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------- QA datasets ---------------- #\n",
    "\n",
    "# PubMedQA (Step 4) – AI-in-healthcare / clinicalAI QA\n",
    "add_dataset_if_exists(\n",
    "    group=\"qa_generation\",\n",
    "    key=\"pubmed_qa\",\n",
    "    path=\"datasets/cleaned/qa_pubmed.json\",\n",
    "    relevance=\"Medium–High (scientific/medical reasoning)\",\n",
    "    use_case=\"Scientific Q&A, useful for AI-in-healthcare / medical study notes\",\n",
    "    dtype=\"medical_scientific_qa\",\n",
    ")\n",
    "\n",
    "# StackOverflow ML Q&A\n",
    "add_dataset_if_exists(\n",
    "    group=\"qa_generation\",\n",
    "    key=\"stackoverflow_ml_250\",\n",
    "    path=\"datasets/cleaned/qa_stackoverflow_ml_250.json\",\n",
    "    relevance=\"High (practical ML/DS programming)\",\n",
    "    use_case=\"Practical questions about ML libraries, Python, model implementation\",\n",
    "    dtype=\"ml_programming_qa\",\n",
    ")\n",
    "\n",
    "# Stackoverflow ML Interview Q&A\n",
    "add_dataset_if_exists(\n",
    "    group=\"qa_generation\",\n",
    "    key=\"aiml_concepts_400\",\n",
    "    path=\"datasets/cleaned/qa_aiml_concepts_400.json\",\n",
    "    relevance=\"High (ML theory + ML tooling)\",\n",
    "    use_case=\"Conceptual ML/NLP questions and ML library Q&A\",\n",
    "    dtype=\"conceptual_and_tooling_qa\",\n",
    ")\n",
    "\n",
    "# ---------------- Totals & save ---------------- #\n",
    "\n",
    "total_items = 0\n",
    "for group_dict in summary[\"datasets\"].values():\n",
    "    for ds_meta in group_dict.values():\n",
    "        total_items += ds_meta.get(\"count\", 0)\n",
    "\n",
    "summary[\"dataset_overview\"][\"total_items\"] = total_items\n",
    "summary[\"dataset_overview\"][\"total_datasets\"] = sum(\n",
    "    len(group_dict) for group_dict in summary[\"datasets\"].values()\n",
    ")\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "summary_path = \"datasets/DATASET_SUMMARY.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATASET SUMMARY CREATED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Summary file: {summary_path}\")\n",
    "print(\"Included datasets:\")\n",
    "for group_name, group in summary[\"datasets\"].items():\n",
    "    for key, meta in group.items():\n",
    "        print(f\" - [{group_name}] {key}: {meta['count']} items -> {meta['file']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpfinal.",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
